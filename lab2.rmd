---
title: "Lab2"
author: "jche827"
date: "5/08/2021"
output: html_document
---

```{r, echo=FALSE}
knitr::opts_chunk$set(comment = "")
```

# The Data Set

The data for this lab consists of open data from the NZ transport agency, Waka Kotahi.

# Acquiring the Data

## 1. CSV Download

```{r, eval=FALSE}
download.file("https://opendata.arcgis.com/api/v3/datasets/b90f8908910f44a493c6501c3565ed2d_0/downloads/data?format=csv&spatialRefId=2193", "traffic-monitoring-sites.csv")
```

## 2. HTML Download
```{r, eval=FALSE}
download.file("https://services.arcgis.com/CXBb7LAjgIIdcsPt/arcgis/rest/services/TMS_Telemetry_Sites/FeatureServer/0/query?outFields=*&where=1%3D1","traffic-daily-counts.html")
```

## 3. JSON
```{r, eval=FALSE}
download.file("https://services.arcgis.com/CXBb7LAjgIIdcsPt/arcgis/rest/services/TMS_Telemetry_Sites/FeatureServer/0/query?where=1%3D1&objectIds=&time=&resultType=none&outFields=*&returnIdsOnly=false&returnUniqueIdsOnly=false&returnCountOnly=false&returnDistinctValues=false&cacheHint=false&orderByFields=&groupByFieldsForStatistics=&outStatistics=&having=&resultOffset=&resultRecordCount=&sqlFormat=none&f=pjson&token=", "traffic-daily-counts.json")
```

# Tasks
## 4. Read the CSV file into an R data frame
```{r}
x <- read.csv("traffic-monitoring-sites.csv")
head(x)
```

## 5. Read the HTML file into R and generate a data frame.

```{r}
library(xml2)
html <- read_html("traffic-daily-counts.html")

# first we use xml_find_all to find the sites on the 3rd row and 2nd cell of the table.
x <- xml_find_all(html, "//table/tr[3]/td[2]")
site <- xml_text(x)[-1]

# we use the same method to find the class and count values
y <- xml_find_all(html, "//table/tr[6]/td[2]")
class <- xml_text(y)[-1]
z <- xml_find_all(html, "//table/tr[10]/td[2]")
count <- xml_text(z)[-1]

# combine our 3 columns into the same dataframe
html_df <- data.frame(site, class, count)
html_df
```

## 6. Read the data from the JSON file into an R data frame similar to the one below
```{r}
library(jsonlite)

# we take the features of the JSON as our attributes in the new dataframe
json <- fromJSON("traffic-daily-counts.json")[["features"]]$attributes
jsondf <- data.frame(json)
head(jsondf)
```

## 7. Write R code to answer the following questions:
How many different days are in the data set? 
```{r}
unique(factor(jsondf$startDate))->x
# the length of all unique start dates are the number of dates
length(x)
```
How many different sites?
```{r}
unique(factor(jsondf$siteID))->y
# the length of all unique site IDs are the number of sites
length(y)
```
How are the counts distributed?
```{r}
# a plot will easily show us the distribution of counts in the traffic json file
plot(density(jsondf$trafficCount), main = "Count distribution")
```







The counts are distributed mainly at 0, and as the count increases it's density(occurances) decrease



What about if we square-root the counts?
```{r}
# adding a column called scount equal to the root of the counts
jsondf$scount <- sqrt(jsondf$trafficCount)

plot(density(jsondf$scount), main = "sCount distribution")
```









The data is less skewed after applying the sqrt to the counts, as there is another small peak at around scount=50.



What about if we split the data into light and heavy counts?
```{r}
# subsetting into heave and light classes
classL = subset(jsondf, classWeight=="Light")
classH = subset(jsondf, classWeight=="Heavy")

# plot for heavy
plot(density(jsondf$trafficCount), main="Heavy class Count distribution")
```
```{r}
# plot for light
plot(density(classL$trafficCount), main="Light class Count distribution")
```









There is still a huge distribution of heavy counts at 0. However in light counts, the data is less skewed towards 0 and has a significant portion at around count=2500 - 12500. 

## 8. Create a new variable scount which contains the square-root of the counts.

```{r}
jsondf$scount <- sqrt(jsondf$trafficCount)

# view our new column (at the bottom)
head(jsondf)
```

## 9. Split the data into training (90%) and test (10%) sets and fit two models using the training data: one that predicts scount from the overall mean of scount and one that predicts scount from class.


Defining the same RMSE function from lab1 and lab0

```{r}
RMSE <- function(obs, pred) {
  sqrt(mean((obs - pred)^2))
}
```

```{r}
index <- sample(rep(1:10, length.out=nrow(jsondf)))
test <- jsondf[index == 1,]
train <- jsondf[index > 1,]

# scount from overall mean of scount in training set
obs <- test$scount
smean <- mean(train$scount)

# scount from class
lmfit1 <- lm(scount ~ classWeight, train)
sclass <- predict(lmfit1, test)
```
### Calculate the RMSE for both models.

Comparing the performance of these two models by their RMSE outputs. 

```{r}
# RMSE value for scount from overall mean of scount
RMSE(obs, smean)
# RMSE value of scount from class
RMSE(obs, sclass)
```

### Could we fit a model that includes siteRef as a predictor as well ?
```{r, eval=FALSE}
lmfit <- lm(scount ~ classWeight * SiteRef, train)
ssite <- predict(lmfit, test)
```
No, because this gives an error in the code, factor SiteRef has new levels. To use siteref as a predictor in our model, we need to remove some levels.



## 10. Create a plot that shows the predictions of both models against the data.
```{r}
plot(scount ~ jitter(as.numeric(factor(classWeight))), test, xlab="class", axes=FALSE)
axis(2)
axis(1, at=as.numeric(unique(factor(test$classWeight))), label=unique(factor(test$classWeight)))
abline(h=smean, col="green")
points(as.numeric(unique(factor(test$classWeight))), predict(lmfit1, data.frame(classWeight=unique(factor(test$classWeight)))),pch=16, col="red")
```

## Summary

In this lab, we took data using webscraping techniques and acquired data in csv, html and json types. We extracted data from html tables by looking at the raw table and indexing the values we need. We then created dataframes from html and json files this way, where we had previously only dont with csv files. We then looked at the different distribution of count and scounts. We then fit two simple models using a training set and compared their performance on a test set. A simple model that predicts the vehicle count based on vehicle class performed better than a model based on the overall mean count, though this still leaves a lot of variation in the data unexplained.
