---
title: "Lab6"
author: "jche827"
date: "18/09/2021"
output: html_document
---

# The Data Set

The data source for this lab is from UCI Machine Learning Repository. We are going to use the Automobile data set. 
# Tasks

# Import

## 1. Produce a tidier file automobile.csv
```{r}
automobileoriginal <- read.csv("automobile-original.csv")
automobilesubset <- read.csv("automobile-subset.csv")

head(automobileoriginal)
head(automobilesubset)
```
### Write R code that can reproduce the second data set from the first one
```{r}
automobile <- automobileoriginal[,-c(9)]
automobile[automobile=='?'] <- NA
automobile <- na.omit(automobile)
write.csv(automobile,"automobile.csv", row.names = FALSE)
```
### Ensure that the values are identical to those from automobile-subset.csv
```{r}
automobile <- read.csv("automobile.csv")
automobilesubset <- read.csv("automobile-subset.csv")
all(automobile == automobilesubset) 
```
# Explore
```{r}
dim(automobile)
```
## 2. Write R code to answer the following questions:

### What is the mean price (price) of all vehicles?
```{r}
mean(automobile$price)
```
### How many vehicles have 4 doors (num.of.doors)?
```{r}
occurences<-table(unlist(automobile$num.of.doors))
occurences
```
#### There are 95 four doored cars

### What are the different engine types (engine.type) among the observations?
```{r}
occurences2 <- table(unlist(automobile$engine.type))
occurences2
```
#### the engine types are: dohc, ohc, l, ohcf and ohcl

### How many vehicles have a price (price) higher than $20000?
```{r}
occurences3 <- table(unlist(automobile$price > 20000))
occurences3
```
#### 13 cars are priced over $20k

### What is the mean price (price) for “4wd” (drive.wheels)
```{r}
mean(automobile[automobile$drive.wheels == "4wd", 25])
```
#### The mean price for 4wd are $10,421

## 3. Produce pairwise scatterplots between variables normalized.losses, wheel.base, peak.rpm and price.
```{r}
pairs(automobile[, c('normalized.losses', 'wheel.base', 'peak.rpm', 'price')])
```
# Linear regression

## 4. Produce the full linear regression model with all variables included
```{r}
r = lm(price ~ ., data=automobile)
summary(r)
```
### Comment on the outcome

There are many seem to cause the linear regression to fail, i.e., some coefficients may become NA. These include: engine.typeohcf, num.of.cylindersthree and fuel.systemidi. the adjusted R squared value is 96.07%, which is a high result although many variables are not extremely significant (p-value > 5%). 

## 5. Remove any variable(s) that seem to cause the linear regression to fail. Repeat this until you can produce a meaningful “full” linear regression model
```{r}
# removing num.of.cylinders, as some values cause the linear regression to fail
r2 = lm(price ~ . - num.of.cylinders, data=automobile)
summary(r2)
```
The adjusted R squared value improves (to 96.08%)
```{r}
# removing engine.type, as some cause they lm to fail. 
r3 = lm(price ~ . - num.of.cylinders -engine.type, data=automobile)
summary(r3)
```
The adjusted R square value decerases to 95.75%. I believe this is due to removal of overfitted values when the NA values we're removed. The values we have removed were mostly p>5% anyway, so this should be a more accurate regression model.
```{r}
# repeat previous step, removing fuel.system
r4 = lm(price ~ . - num.of.cylinders -engine.type -fuel.system, data=automobile)
summary(r4)
```
```{r}
# removing variables with low significance 
r5 = lm(price ~ . - num.of.cylinders -engine.type -fuel.system -symboling -normalized.losses -fuel.type -drive.wheels -stroke -compression.ratio -horsepower -peak.rpm -city.mpg -highway.mpg -num.of.doors -width -engine.size -bore, data=automobile)
summary(r5)
```
The final model has an adjusted r squared value of 95.21%. The majority of values are p-value significant <5%, and I think this is the best model with the most relevant varibles and without the irrelevant ones. 

### How many variables are deemed to be significant by the t-tests (with a p-value less than 0.05)?

24 variables are significant (out of 27) with a p vvalue <5%. If you consider the makeporsche variable to be significant as it has a p value of 0.05158, which is very close to significance, then 25 variables are significant. 

## 6. Apply your “full” linear regression model to the data and compute the resulting mean squared error (MSE)
```{r}
mse = function(y1, y2) mean( (y1 - y2)^2 )
sqerr = mse(predict(r5, automobile), automobile$price)
sqerr

# square root of mean squared error:
sqrt(sqerr)
```
# Subset selection
```{r}
# load libraries
library(leaps)
library(glmnet)
```
## 7. Produce a subset linear regression model, using the backward selection and the AIC
```{r}
r.bwd = regsubsets(price ~ . - num.of.cylinders -engine.type -fuel.system -symboling -normalized.losses -fuel.type -drive.wheels -stroke -compression.ratio -horsepower -peak.rpm -city.mpg -highway.mpg -num.of.doors -width -engine.size -bore, data=automobile, nvmax = 30, method="backward")
coef(r.bwd, 1:3)
r.s = summary(r.bwd)
names(r.s)
(bic = r.s$bic)
(j = which.min(bic))
(beta = coef(r.bwd, j))
k <- c()
for (i in 1:length(bic)){
  k[i] <- length(coef(r.bwd, i))-1
}
aic = bic - log(nrow(automobile))*k + 2*k
plot(aic)
```

## 8. Apply the AIC-selected model to the data and compute the resulting MSE
```{r}
automobile.matrix = model.matrix(price ~ . - num.of.cylinders -engine.type -fuel.system -symboling -normalized.losses -fuel.type -drive.wheels -stroke -compression.ratio -horsepower -peak.rpm -city.mpg -highway.mpg -num.of.doors -width -engine.size -bore, data=automobile)

bwj.matrix = automobile.matrix[,names(beta)]
(yhat = drop(bwj.matrix %*% beta))  
resid = automobile$price - yhat
(mse = mean(resid^2))
```


## 9. Create a plot that shows the predictions of your AIC-selected model against the response variable, using different colors for different levels of drive.wheels.
```{r}
plot(r.bwd, col=factor(automobile$drive.wheels))
```

# Lasso

## 10. Compute the Lasso model.
```{r}
x = automobile.matrix[,-1]     # remove the intercept term
y = automobile$price
(r.lasso = glmnet(x, y, alpha=1)) 
```

## 11. Create a coefficient profile plot of the coefficient paths that varies with the value of λ (or log(λ))
```{r}
names(r.lasso)
coef(r.lasso)[,10]
```

## 12. Choose 5 different λ-values within a seemingly reasonable range
```{r}
# lambda = 2
r.lasso1 = glmnet(x, y, alpha=1, lambda = 2)
(yhat1 = drop(predict(r.lasso, s=2, alpha=1, newx=x)))  
resid1 = automobile$price - yhat1 

# lambda = 4
r.lasso2 = glmnet(x, y, alpha=1, lambda = 4)
(yhat2 = drop(predict(r.lasso, s=4, alpha=1, newx=x))) 
resid2 = automobile$price - yhat2 

# lambda = 6
r.lasso3 = glmnet(x, y, alpha=1, lambda = 6)
(yhat3 = drop(predict(r.lasso, s=6, alpha=1, newx=x)))  
resid3 = automobile$price - yhat3 

# lambda = 8
r.lasso4 = glmnet(x, y, alpha=1, lambda = 8)
(yhat4 = drop(predict(r.lasso, s=8, alpha=1, newx=x))) 
resid4 = automobile$price - yhat4 

# lambda = 10
r.lasso5 = glmnet(x, y, alpha=1, lambda = 10)
(yhat5 = drop(predict(r.lasso, s=10, alpha=1, newx=x)))  
resid5 = automobile$price - yhat5 
```
### Compute the MSEs of the corresponding 5 Lasso subset models
```{r}
# MSE for lambda = 2
(mse1 = mean(resid1^2))

# MSE for lambda = 4
(mse2 = mean(resid2^2))

# MSE for lambda = 6
(mse3 = mean(resid3^2))

# MSE for lambda = 8
(mse4 = mean(resid4^2))

# MSE for lambda = 10
(mse5 = mean(resid5^2))
```


### Write R code to find out how many variables are included in each Lasso subset model
```{r}
# number of variables in lambda = 2
paste(r.lasso1$df-1)
# number of variables in lambda = 4
paste(r.lasso2$df-1)
# number of variables in lambda = 6
paste(r.lasso3$df-1)
# number of variables in lambda = 8
paste(r.lasso4$df-1)
# number of variables in lambda = 10
paste(r.lasso5$df-1)

```
The mse gradually rises as the lambda increases. At higher lambdas such as 30+, the number of variables begins to decrease from 25 to 24, 23, etc.

# Summary

In this lab we explored different methods of linear regression. We first cleaned the data by removing the missing values represetned by "?". We then practices basic queries in R to find stuff like the mean, counts and different types of variables in the data. We then used pairwise scatter plots to show some visualization.

Linear regression models were used to and comapred with each other using different variables. We removed variables we thought were insignificant, or would cause the linear regression model to fail. Backwards selection and the AIC were used for subset selection, and we computed the lasso model which selects variables based on the lambda value.. We then chose λ values between a range and computed the mean squared errors for them. We looked at how different lambdas affected the variable selection.











